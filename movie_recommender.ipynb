{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 个性化推荐\n",
    "本项目使用文本卷积神经网络，并使用[`MovieLens`](https://grouplens.org/datasets/movielens/)数据集完成电影推荐的任务。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐系统在日常的网络应用中无处不在，比如网上购物、网上买书、新闻app、社交网络、音乐网站、电影网站等等等等，有人的地方就有推荐。根据个人的喜好，相同喜好人群的习惯等信息进行个性化的内容推荐。比如打开新闻类的app，因为有了个性化的内容，每个人看到的新闻首页都是不一样的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这当然是很有用的，在信息爆炸的今天，获取信息的途径和方式多种多样，人们花费时间最多的不再是去哪获取信息，而是要在众多的信息中寻找自己感兴趣的，这就是信息超载问题。为了解决这个问题，推荐系统应运而生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "协同过滤是推荐系统应用较广泛的技术，该方法搜集用户的历史记录、个人喜好等信息，计算与其他用户的相似度，利用相似用户的评价来预测目标用户对特定项目的喜好程度。优点是会给用户推荐未浏览过的项目，缺点呢，对于新用户来说，没有任何与商品的交互记录和个人喜好等信息，存在冷启动问题，导致模型无法找到相似的用户或商品。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了解决冷启动的问题，通常的做法是对于刚注册的用户，要求用户先选择自己感兴趣的话题、群组、商品、性格、喜欢的音乐类型等信息，比如豆瓣FM：\n",
    "<img src=\"assets/IMG_6242_300.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 下载数据集\n",
    "运行下面代码把[`数据集`](http://files.grouplens.org/datasets/movielens/ml-1m.zip)下载下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from tensorflow.python.ops import math_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import hashlib\n",
    "\n",
    "def _unzip(save_path, _, database_name, data_path):\n",
    "    \"\"\"\n",
    "    Unzip wrapper with the same interface as _ungzip\n",
    "    :param save_path: The path of the gzip files\n",
    "    :param database_name: Name of database\n",
    "    :param data_path: Path to extract to\n",
    "    :param _: HACK - Used to have to same interface as _ungzip\n",
    "    \"\"\"\n",
    "    print('Extracting {}...'.format(database_name))\n",
    "    with zipfile.ZipFile(save_path) as zf:\n",
    "        zf.extractall(data_path)\n",
    "\n",
    "def download_extract(database_name, data_path):\n",
    "    \"\"\"\n",
    "    Download and extract database\n",
    "    :param database_name: Database name\n",
    "    \"\"\"\n",
    "    DATASET_ML1M = 'ml-1m'\n",
    "\n",
    "    if database_name == DATASET_ML1M:\n",
    "        url = 'http://files.grouplens.org/datasets/movielens/ml-1m.zip'\n",
    "        hash_code = 'c4d9eecfca2ab87c1945afe126590906'\n",
    "        extract_path = os.path.join(data_path, 'ml-1m')\n",
    "        save_path = os.path.join(data_path, 'ml-1m.zip')\n",
    "        extract_fn = _unzip\n",
    "\n",
    "    if os.path.exists(extract_path):\n",
    "        print('Found {} Data'.format(database_name))\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(save_path):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Downloading {}'.format(database_name)) as pbar:\n",
    "            urlretrieve(\n",
    "                url,\n",
    "                save_path,\n",
    "                pbar.hook)\n",
    "\n",
    "    assert hashlib.md5(open(save_path, 'rb').read()).hexdigest() == hash_code, \\\n",
    "        '{} file is corrupted.  Remove the file and try again.'.format(save_path)\n",
    "\n",
    "    os.makedirs(extract_path)\n",
    "    try:\n",
    "        extract_fn(save_path, extract_path, database_name, data_path)\n",
    "    except Exception as err:\n",
    "        shutil.rmtree(extract_path)  # Remove extraction folder if there is an error\n",
    "        raise err\n",
    "\n",
    "    print('Done.')\n",
    "    # Remove compressed data\n",
    "#     os.remove(save_path)\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    \"\"\"\n",
    "    Handle Progress Bar while Downloading\n",
    "    \"\"\"\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        \"\"\"\n",
    "        A hook function that will be called once on establishment of the network connection and\n",
    "        once after each block read thereafter.\n",
    "        :param block_num: A count of blocks transferred so far\n",
    "        :param block_size: Block size in bytes\n",
    "        :param total_size: The total size of the file. This may be -1 on older FTP servers which do not return\n",
    "                            a file size in response to a retrieval request.\n",
    "        \"\"\"\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found ml-1m Data\n"
     ]
    }
   ],
   "source": [
    "data_dir = './'\n",
    "download_extract('ml-1m', data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先来看看数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目使用的是MovieLens 1M 数据集，包含6000个用户在近4000部电影上的1亿条评论。\n",
    "\n",
    "数据集分为三个文件：用户数据users.dat，电影数据movies.dat和评分数据ratings.dat。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用户数据\n",
    "分别有用户ID、性别、年龄、职业ID和邮编等字段。\n",
    "\n",
    "数据中的格式：UserID::Gender::Age::Occupation::Zip-code\n",
    "\n",
    "- Gender is denoted by a \"M\" for male and \"F\" for female\n",
    "- Age is chosen from the following ranges:\n",
    "\n",
    "\t*  1:  \"Under 18\"\n",
    "\t* 18:  \"18-24\"\n",
    "\t* 25:  \"25-34\"\n",
    "\t* 35:  \"35-44\"\n",
    "\t* 45:  \"45-49\"\n",
    "\t* 50:  \"50-55\"\n",
    "\t* 56:  \"56+\"\n",
    "\n",
    "- Occupation is chosen from the following choices:\n",
    "\n",
    "\t*  0:  \"other\" or not specified\n",
    "\t*  1:  \"academic/educator\"\n",
    "\t*  2:  \"artist\"\n",
    "\t*  3:  \"clerical/admin\"\n",
    "\t*  4:  \"college/grad student\"\n",
    "\t*  5:  \"customer service\"\n",
    "\t*  6:  \"doctor/health care\"\n",
    "\t*  7:  \"executive/managerial\"\n",
    "\t*  8:  \"farmer\"\n",
    "\t*  9:  \"homemaker\"\n",
    "\t* 10:  \"K-12 student\"\n",
    "\t* 11:  \"lawyer\"\n",
    "\t* 12:  \"programmer\"\n",
    "\t* 13:  \"retired\"\n",
    "\t* 14:  \"sales/marketing\"\n",
    "\t* 15:  \"scientist\"\n",
    "\t* 16:  \"self-employed\"\n",
    "\t* 17:  \"technician/engineer\"\n",
    "\t* 18:  \"tradesman/craftsman\"\n",
    "\t* 19:  \"unemployed\"\n",
    "\t* 20:  \"writer\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>OccupationID</th>\n",
       "      <th>Zip-code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>48067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>70072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>55117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>02460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>55455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID Gender  Age  OccupationID Zip-code\n",
       "0       1      F    1            10    48067\n",
       "1       2      M   56            16    70072\n",
       "2       3      M   25            15    55117\n",
       "3       4      M   45             7    02460\n",
       "4       5      M   25            20    55455"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_title = ['UserID', 'Gender', 'Age', 'OccupationID', 'Zip-code']\n",
    "users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出UserID、Gender、Age和Occupation都是类别字段，其中邮编字段是我们不使用的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 电影数据\n",
    "分别有电影ID、电影名和电影风格等字段。\n",
    "\n",
    "数据中的格式：MovieID::Title::Genres\n",
    "\n",
    "- Titles are identical to titles provided by the IMDB (including\n",
    "year of release)\n",
    "- Genres are pipe-separated and are selected from the following genres:\n",
    "\n",
    "\t* Action\n",
    "\t* Adventure\n",
    "\t* Animation\n",
    "\t* Children's\n",
    "\t* Comedy\n",
    "\t* Crime\n",
    "\t* Documentary\n",
    "\t* Drama\n",
    "\t* Fantasy\n",
    "\t* Film-Noir\n",
    "\t* Horror\n",
    "\t* Musical\n",
    "\t* Mystery\n",
    "\t* Romance\n",
    "\t* Sci-Fi\n",
    "\t* Thriller\n",
    "\t* War\n",
    "\t* Western\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Animation|Children's|Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "      <td>Adventure|Children's|Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "      <td>Comedy|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                               Title                        Genres\n",
       "0        1                    Toy Story (1995)   Animation|Children's|Comedy\n",
       "1        2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
       "2        3             Grumpier Old Men (1995)                Comedy|Romance\n",
       "3        4            Waiting to Exhale (1995)                  Comedy|Drama\n",
       "4        5  Father of the Bride Part II (1995)                        Comedy"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_title = ['MovieID', 'Title', 'Genres']\n",
    "movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MovieID是类别字段，Title是文本，Genres也是类别字段"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评分数据\n",
    "分别有用户ID、电影ID、评分和时间戳等字段。\n",
    "\n",
    "数据中的格式：UserID::MovieID::Rating::Timestamp\n",
    "\n",
    "- UserIDs range between 1 and 6040 \n",
    "- MovieIDs range between 1 and 3952\n",
    "- Ratings are made on a 5-star scale (whole-star ratings only)\n",
    "- Timestamp is represented in seconds since the epoch as returned by time(2)\n",
    "- Each user has at least 20 ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Rating</th>\n",
       "      <th>timestamps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  MovieID  Rating  timestamps\n",
       "0       1     1193       5   978300760\n",
       "1       1      661       3   978302109\n",
       "2       1      914       3   978301968\n",
       "3       1     3408       4   978300275\n",
       "4       1     2355       5   978824291"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_title = ['UserID','MovieID', 'Rating', 'timestamps']\n",
    "ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评分字段Rating就是我们要学习的targets，时间戳字段我们不使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 来说说数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UserID、Occupation和MovieID不用变。\n",
    "- Gender字段：需要将‘F’和‘M’转换成0和1。\n",
    "- Age字段：要转成7个连续数字0~6。\n",
    "- Genres字段：是分类字段，要转成数字。首先将Genres中的类别转成字符串到数字的字典，然后再将每个电影的Genres字段转成数字列表，因为有些电影是多个Genres的组合。\n",
    "- Title字段：处理方式跟Genres字段一样，首先创建文本到数字的字典，然后将Title中的描述转成数字的列表。另外Title中的年份也需要去掉。\n",
    "- Genres和Title字段需要将长度统一，这样在神经网络中方便处理。空白部分用‘< PAD >’对应的数字填充。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    #读取User数据\n",
    "    users_title = ['UserID', 'Gender', 'Age', 'JobID', 'Zip-code']\n",
    "    users = pd.read_table('./ml-1m/users.dat', sep='::', header=None, names=users_title, engine = 'python')\n",
    "    users = users.filter(regex='UserID|Gender|Age|JobID')\n",
    "    users_orig = users.values\n",
    "    #改变User数据中性别和年龄\n",
    "    gender_map = {'F':0, 'M':1}\n",
    "    users['Gender'] = users['Gender'].map(gender_map)\n",
    "\n",
    "    age_map = {val:ii for ii,val in enumerate(set(users['Age']))}\n",
    "    users['Age'] = users['Age'].map(age_map)\n",
    "\n",
    "    #读取Movie数据集\n",
    "    movies_title = ['MovieID', 'Title', 'Genres']\n",
    "    movies = pd.read_table('./ml-1m/movies.dat', sep='::', header=None, names=movies_title, engine = 'python')\n",
    "    movies_orig = movies.values\n",
    "    #将Title中的年份去掉\n",
    "    pattern = re.compile(r'^(.*)\\((\\d+)\\)$')\n",
    "\n",
    "    title_map = {val:pattern.match(val).group(1) for ii,val in enumerate(set(movies['Title']))}\n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #电影类型转数字字典\n",
    "    genres_set = set()\n",
    "    for val in movies['Genres'].str.split('|'):\n",
    "        genres_set.update(val)\n",
    "\n",
    "    genres_set.add('<PAD>')\n",
    "    genres2int = {val:ii for ii, val in enumerate(genres_set)}\n",
    "\n",
    "    #将电影类型转成等长数字列表，长度是18\n",
    "    genres_map = {val:[genres2int[row] for row in val.split('|')] for ii,val in enumerate(set(movies['Genres']))}\n",
    "\n",
    "    for key in genres_map:\n",
    "        for cnt in range(max(genres2int.values()) - len(genres_map[key])):\n",
    "            genres_map[key].insert(len(genres_map[key]) + cnt,genres2int['<PAD>'])\n",
    "    \n",
    "    movies['Genres'] = movies['Genres'].map(genres_map)\n",
    "\n",
    "    #电影Title转数字字典\n",
    "    title_set = set()\n",
    "    for val in movies['Title'].str.split():\n",
    "        title_set.update(val)\n",
    "    \n",
    "    title_set.add('<PAD>')\n",
    "    title2int = {val:ii for ii, val in enumerate(title_set)}\n",
    "\n",
    "    #将电影Title转成等长数字列表，长度是15\n",
    "    title_count = 15\n",
    "    title_map = {val:[title2int[row] for row in val.split()] for ii,val in enumerate(set(movies['Title']))}\n",
    "    \n",
    "    for key in title_map:\n",
    "        for cnt in range(title_count - len(title_map[key])):\n",
    "            title_map[key].insert(len(title_map[key]) + cnt,title2int['<PAD>'])\n",
    "    \n",
    "    movies['Title'] = movies['Title'].map(title_map)\n",
    "\n",
    "    #读取评分数据集\n",
    "    ratings_title = ['UserID','MovieID', 'ratings', 'timestamps']\n",
    "    ratings = pd.read_table('./ml-1m/ratings.dat', sep='::', header=None, names=ratings_title, engine = 'python')\n",
    "    ratings = ratings.filter(regex='UserID|MovieID|ratings')\n",
    "\n",
    "    #合并三个表\n",
    "    data = pd.merge(pd.merge(ratings, users), movies)\n",
    "    \n",
    "    #将数据分成X和y两张表\n",
    "    target_fields = ['ratings']\n",
    "    features_pd, targets_pd = data.drop(target_fields, axis=1), data[target_fields]\n",
    "    \n",
    "    features = features_pd.values\n",
    "    targets_values = targets_pd.values\n",
    "    \n",
    "    return title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据并保存到本地"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- title_count：Title字段的长度（15）\n",
    "- title_set：Title文本的集合\n",
    "- genres2int：电影类型转数字的字典\n",
    "- features：是输入X\n",
    "- targets_values：是学习目标y\n",
    "- ratings：评分数据集的Pandas对象\n",
    "- users：用户数据集的Pandas对象\n",
    "- movies：电影数据的Pandas对象\n",
    "- data：三个数据集组合在一起的Pandas对象\n",
    "- movies_orig：没有做数据处理的原始电影数据\n",
    "- users_orig：没有做数据处理的原始用户数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:7: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  import sys\n",
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:19: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py:64: FutureWarning: read_table is deprecated, use read_csv instead.\n"
     ]
    }
   ],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = load_data()\n",
    "\n",
    "pickle.dump((title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig), open('preprocess.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理后的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>JobID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserID  Gender  Age  JobID\n",
       "0       1       0    0     10\n",
       "1       2       1    5     16\n",
       "2       3       1    6     15\n",
       "3       4       1    2      7\n",
       "4       5       1    6     20"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MovieID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[4723, 2074, 543, 543, 543, 543, 543, 543, 543...</td>\n",
       "      <td>[8, 7, 5, 11, 11, 11, 11, 11, 11, 11, 11, 11, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[1000, 543, 543, 543, 543, 543, 543, 543, 543,...</td>\n",
       "      <td>[0, 7, 16, 11, 11, 11, 11, 11, 11, 11, 11, 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[2868, 5016, 3646, 543, 543, 543, 543, 543, 54...</td>\n",
       "      <td>[5, 2, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[3934, 1910, 5112, 543, 543, 543, 543, 543, 54...</td>\n",
       "      <td>[5, 14, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[4582, 2350, 1741, 1720, 373, 1537, 543, 543, ...</td>\n",
       "      <td>[5, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MovieID                                              Title  \\\n",
       "0        1  [4723, 2074, 543, 543, 543, 543, 543, 543, 543...   \n",
       "1        2  [1000, 543, 543, 543, 543, 543, 543, 543, 543,...   \n",
       "2        3  [2868, 5016, 3646, 543, 543, 543, 543, 543, 54...   \n",
       "3        4  [3934, 1910, 5112, 543, 543, 543, 543, 543, 54...   \n",
       "4        5  [4582, 2350, 1741, 1720, 373, 1537, 543, 543, ...   \n",
       "\n",
       "                                              Genres  \n",
       "0  [8, 7, 5, 11, 11, 11, 11, 11, 11, 11, 11, 11, ...  \n",
       "1  [0, 7, 16, 11, 11, 11, 11, 11, 11, 11, 11, 11,...  \n",
       "2  [5, 2, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,...  \n",
       "3  [5, 14, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11...  \n",
       "4  [5, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1,\n",
       "       list([4723, 2074, 543, 543, 543, 543, 543, 543, 543, 543, 543, 543, 543, 543, 543]),\n",
       "       list([8, 7, 5, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从本地读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_count, title_set, genres2int, features, targets_values, ratings, users, movies, data, movies_orig, users_orig = pickle.load(open('preprocess.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型设计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/model.001.jpeg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过研究数据集中的字段类型，我们发现有一些是类别字段，通常的处理是将这些字段转成one hot编码，但是像UserID、MovieID这样的字段就会变成非常的稀疏，输入的维度急剧膨胀，这是我们不愿意见到的，毕竟我这小笔记本不像大厂动辄能处理数以亿计维度的输入：）\n",
    "\n",
    "所以在预处理数据时将这些字段转成了数字，我们用这个数字当做嵌入矩阵的索引，在网络的第一层使用了嵌入层，维度是（N，32）和（N，16）。\n",
    "\n",
    "电影类型的处理要多一步，有时一个电影有多个电影类型，这样从嵌入矩阵索引出来是一个（n，32）的矩阵，因为有多个类型嘛，我们要将这个矩阵求和，变成（1，32）的向量。\n",
    "\n",
    "电影名的处理比较特殊，没有使用循环神经网络，而是用了文本卷积网络，下文会进行说明。\n",
    "\n",
    "从嵌入层索引出特征以后，将各特征传入全连接层，将输出再次传入全连接层，最终分别得到（1，200）的用户特征和电影特征两个特征向量。\n",
    "\n",
    "我们的目的就是要训练出用户特征和电影特征，在实现推荐功能时使用。得到这两个特征以后，就可以选择任意的方式来拟合评分了。我使用了两种方式，一个是上图中画出的将两个特征做向量乘法，将结果与真实评分做回归，采用MSE优化损失。因为本质上这是一个回归问题，另一种方式是，将两个特征作为输入，再次传入全连接层，输出一个值，将输出值回归到真实评分，采用MSE优化损失。\n",
    "\n",
    "实际上第二个方式的MSE loss在0.8附近，第一个方式在1附近，5次迭代的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本卷积网络\n",
    "网络看起来像下面这样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/text_cnn.png\"/>\n",
    "图片来自Kim Yoon的论文：[`Convolutional Neural Networks for Sentence Classification`](https://arxiv.org/abs/1408.5882)\n",
    "\n",
    "将卷积神经网络用于文本的文章建议你阅读[`Understanding Convolutional Neural Networks for NLP`](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络的第一层是词嵌入层，由每一个单词的嵌入向量组成的嵌入矩阵。下一层使用多个不同尺寸（窗口大小）的卷积核在嵌入矩阵上做卷积，窗口大小指的是每次卷积覆盖几个单词。这里跟对图像做卷积不太一样，图像的卷积通常用2x2、3x3、5x5之类的尺寸，而文本卷积要覆盖整个单词的嵌入向量，所以尺寸是（单词数，向量维度），比如每次滑动3个，4个或者5个单词。第三层网络是max pooling得到一个长向量，最后使用dropout做正则化，最终得到了电影Title的特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def save_params(params):\n",
    "    \"\"\"\n",
    "    Save parameters to file\n",
    "    \"\"\"\n",
    "    pickle.dump(params, open('params.p', 'wb'))\n",
    "\n",
    "\n",
    "def load_params():\n",
    "    \"\"\"\n",
    "    Load parameters from file\n",
    "    \"\"\"\n",
    "    return pickle.load(open('params.p', mode='rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#嵌入矩阵的维度\n",
    "embed_dim = 32\n",
    "#用户ID个数\n",
    "uid_max = max(features.take(0,1)) + 1 # 6040\n",
    "#性别个数\n",
    "gender_max = max(features.take(2,1)) + 1 # 1 + 1 = 2\n",
    "#年龄类别个数\n",
    "age_max = max(features.take(3,1)) + 1 # 6 + 1 = 7\n",
    "#职业个数\n",
    "job_max = max(features.take(4,1)) + 1# 20 + 1 = 21\n",
    "\n",
    "#电影ID个数\n",
    "movie_id_max = max(features.take(1,1)) + 1 # 3952\n",
    "#电影类型个数\n",
    "movie_categories_max = max(genres2int.values()) + 1 # 18 + 1 = 19\n",
    "#电影名单词个数\n",
    "movie_title_max = len(title_set) # 5216\n",
    "\n",
    "#对电影类型嵌入向量做加和操作的标志，考虑过使用mean做平均，但是没实现mean\n",
    "combiner = \"sum\"\n",
    "\n",
    "#电影名长度\n",
    "sentences_size = title_count # = 15\n",
    "#文本卷积滑动窗口，分别滑动2, 3, 4, 5个单词\n",
    "window_sizes = {2, 3, 4, 5}\n",
    "#文本卷积核数量\n",
    "filter_num = 8\n",
    "\n",
    "#电影ID转下标的字典，数据集中电影ID跟下标不一致，比如第5行的数据电影ID不一定是5\n",
    "movieid2idx = {val[0]:i for i, val in enumerate(movies.values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 5\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "\n",
    "dropout_keep = 0.5\n",
    "# Learning Rate\n",
    "learning_rate = 0.0001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 输入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义输入的占位符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "    uid = tf.placeholder(tf.int32, [None, 1], name=\"uid\")\n",
    "    user_gender = tf.placeholder(tf.int32, [None, 1], name=\"user_gender\")\n",
    "    user_age = tf.placeholder(tf.int32, [None, 1], name=\"user_age\")\n",
    "    user_job = tf.placeholder(tf.int32, [None, 1], name=\"user_job\")\n",
    "    \n",
    "    movie_id = tf.placeholder(tf.int32, [None, 1], name=\"movie_id\")\n",
    "    movie_categories = tf.placeholder(tf.int32, [None, 18], name=\"movie_categories\")\n",
    "    movie_titles = tf.placeholder(tf.int32, [None, 15], name=\"movie_titles\")\n",
    "    targets = tf.placeholder(tf.int32, [None, 1], name=\"targets\")\n",
    "    LearningRate = tf.placeholder(tf.float32, name = \"LearningRate\")\n",
    "    dropout_keep_prob = tf.placeholder(tf.float32, name = \"dropout_keep_prob\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, LearningRate, dropout_keep_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义User的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(uid, user_gender, user_age, user_job):\n",
    "    with tf.name_scope(\"user_embedding\"):\n",
    "        uid_embed_matrix = tf.Variable(tf.random_uniform([uid_max, embed_dim], -1, 1), name = \"uid_embed_matrix\")\n",
    "        uid_embed_layer = tf.nn.embedding_lookup(uid_embed_matrix, uid, name = \"uid_embed_layer\")\n",
    "    \n",
    "        gender_embed_matrix = tf.Variable(tf.random_uniform([gender_max, embed_dim // 2], -1, 1), name= \"gender_embed_matrix\")\n",
    "        gender_embed_layer = tf.nn.embedding_lookup(gender_embed_matrix, user_gender, name = \"gender_embed_layer\")\n",
    "        \n",
    "        age_embed_matrix = tf.Variable(tf.random_uniform([age_max, embed_dim // 2], -1, 1), name=\"age_embed_matrix\")\n",
    "        age_embed_layer = tf.nn.embedding_lookup(age_embed_matrix, user_age, name=\"age_embed_layer\")\n",
    "        \n",
    "        job_embed_matrix = tf.Variable(tf.random_uniform([job_max, embed_dim // 2], -1, 1), name = \"job_embed_matrix\")\n",
    "        job_embed_layer = tf.nn.embedding_lookup(job_embed_matrix, user_job, name = \"job_embed_layer\")\n",
    "    return uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将User的嵌入矩阵一起全连接生成User的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer):\n",
    "    with tf.name_scope(\"user_fc\"):\n",
    "        #第一层全连接\n",
    "        uid_fc_layer = tf.layers.dense(uid_embed_layer, embed_dim, name = \"uid_fc_layer\", activation=tf.nn.relu)\n",
    "        gender_fc_layer = tf.layers.dense(gender_embed_layer, embed_dim, name = \"gender_fc_layer\", activation=tf.nn.relu)\n",
    "        age_fc_layer = tf.layers.dense(age_embed_layer, embed_dim, name =\"age_fc_layer\", activation=tf.nn.relu)\n",
    "        job_fc_layer = tf.layers.dense(job_embed_layer, embed_dim, name = \"job_fc_layer\", activation=tf.nn.relu)\n",
    "        \n",
    "        #第二层全连接\n",
    "        user_combine_layer = tf.concat([uid_fc_layer, gender_fc_layer, age_fc_layer, job_fc_layer], 2)  #(?, 1, 128)\n",
    "        user_combine_layer = tf.contrib.layers.fully_connected(user_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        user_combine_layer_flat = tf.reshape(user_combine_layer, [-1, 200])\n",
    "    return user_combine_layer, user_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义Movie ID的嵌入矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_id_embed_layer(movie_id):\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_id_embed_matrix = tf.Variable(tf.random_uniform([movie_id_max, embed_dim], -1, 1), name = \"movie_id_embed_matrix\")\n",
    "        movie_id_embed_layer = tf.nn.embedding_lookup(movie_id_embed_matrix, movie_id, name = \"movie_id_embed_layer\")\n",
    "    return movie_id_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对电影类型的多个嵌入向量做加和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_categories_layers(movie_categories):\n",
    "    with tf.name_scope(\"movie_categories_layers\"):\n",
    "        movie_categories_embed_matrix = tf.Variable(tf.random_uniform([movie_categories_max, embed_dim], -1, 1), name = \"movie_categories_embed_matrix\")\n",
    "        movie_categories_embed_layer = tf.nn.embedding_lookup(movie_categories_embed_matrix, movie_categories, name = \"movie_categories_embed_layer\")\n",
    "        if combiner == \"sum\":\n",
    "            movie_categories_embed_layer = tf.reduce_sum(movie_categories_embed_layer, axis=1, keep_dims=True)\n",
    "    #     elif combiner == \"mean\":\n",
    "\n",
    "    return movie_categories_embed_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Movie Title的文本卷积网络实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_cnn_layer(movie_titles):\n",
    "    #从嵌入矩阵中得到电影名对应的各个单词的嵌入向量\n",
    "    with tf.name_scope(\"movie_embedding\"):\n",
    "        movie_title_embed_matrix = tf.Variable(tf.random_uniform([movie_title_max, embed_dim], -1, 1), name = \"movie_title_embed_matrix\")\n",
    "        movie_title_embed_layer = tf.nn.embedding_lookup(movie_title_embed_matrix, movie_titles, name = \"movie_title_embed_layer\")\n",
    "        movie_title_embed_layer_expand = tf.expand_dims(movie_title_embed_layer, -1)\n",
    "    \n",
    "    #对文本嵌入层使用不同尺寸的卷积核做卷积和最大池化\n",
    "    pool_layer_lst = []\n",
    "    for window_size in window_sizes:\n",
    "        with tf.name_scope(\"movie_txt_conv_maxpool_{}\".format(window_size)):\n",
    "            filter_weights = tf.Variable(tf.truncated_normal([window_size, embed_dim, 1, filter_num],stddev=0.1),name = \"filter_weights\")\n",
    "            filter_bias = tf.Variable(tf.constant(0.1, shape=[filter_num]), name=\"filter_bias\")\n",
    "            \n",
    "            conv_layer = tf.nn.conv2d(movie_title_embed_layer_expand, filter_weights, [1,1,1,1], padding=\"VALID\", name=\"conv_layer\")\n",
    "            relu_layer = tf.nn.relu(tf.nn.bias_add(conv_layer,filter_bias), name =\"relu_layer\")\n",
    "            \n",
    "            maxpool_layer = tf.nn.max_pool(relu_layer, [1,sentences_size - window_size + 1 ,1,1], [1,1,1,1], padding=\"VALID\", name=\"maxpool_layer\")\n",
    "            pool_layer_lst.append(maxpool_layer)\n",
    "\n",
    "    #Dropout层\n",
    "    with tf.name_scope(\"pool_dropout\"):\n",
    "        pool_layer = tf.concat(pool_layer_lst, 3, name =\"pool_layer\")\n",
    "        max_num = len(window_sizes) * filter_num\n",
    "        pool_layer_flat = tf.reshape(pool_layer , [-1, 1, max_num], name = \"pool_layer_flat\")\n",
    "    \n",
    "        dropout_layer = tf.nn.dropout(pool_layer_flat, dropout_keep_prob, name = \"dropout_layer\")\n",
    "    return pool_layer_flat, dropout_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 将Movie的各个层一起做全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_feature_layer(movie_id_embed_layer, movie_categories_embed_layer, dropout_layer):\n",
    "    with tf.name_scope(\"movie_fc\"):\n",
    "        #第一层全连接\n",
    "        movie_id_fc_layer = tf.layers.dense(movie_id_embed_layer, embed_dim, name = \"movie_id_fc_layer\", activation=tf.nn.relu)\n",
    "        movie_categories_fc_layer = tf.layers.dense(movie_categories_embed_layer, embed_dim, name = \"movie_categories_fc_layer\", activation=tf.nn.relu)\n",
    "    \n",
    "        #第二层全连接\n",
    "        movie_combine_layer = tf.concat([movie_id_fc_layer, movie_categories_fc_layer, dropout_layer], 2)  #(?, 1, 96)\n",
    "        movie_combine_layer = tf.contrib.layers.fully_connected(movie_combine_layer, 200, tf.tanh)  #(?, 1, 200)\n",
    "    \n",
    "        movie_combine_layer_flat = tf.reshape(movie_combine_layer, [-1, 200])\n",
    "    return movie_combine_layer, movie_combine_layer_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-18-dc08624e5fcc>:4: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-20-559a1ee9ce9e>:6: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From <ipython-input-21-bb012f2abe28>:27: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:667: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    #获取输入占位符\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob = get_inputs()\n",
    "    #获取User的4个嵌入向量\n",
    "    uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer = get_user_embedding(uid, user_gender, user_age, user_job)\n",
    "    #得到用户特征\n",
    "    user_combine_layer, user_combine_layer_flat = get_user_feature_layer(uid_embed_layer, gender_embed_layer, age_embed_layer, job_embed_layer)\n",
    "    #获取电影ID的嵌入向量\n",
    "    movie_id_embed_layer = get_movie_id_embed_layer(movie_id)\n",
    "    #获取电影类型的嵌入向量\n",
    "    movie_categories_embed_layer = get_movie_categories_layers(movie_categories)\n",
    "    #获取电影名的特征向量\n",
    "    pool_layer_flat, dropout_layer = get_movie_cnn_layer(movie_titles)\n",
    "    #得到电影特征\n",
    "    movie_combine_layer, movie_combine_layer_flat = get_movie_feature_layer(movie_id_embed_layer, \n",
    "                                                                                movie_categories_embed_layer, \n",
    "                                                                                dropout_layer)\n",
    "    #计算出评分，要注意两个不同的方案，inference的名字（name值）是不一样的，后面做推荐时要根据name取得tensor\n",
    "    with tf.name_scope(\"inference\"):\n",
    "        #将用户特征和电影特征作为输入，经过全连接，输出一个值的方案\n",
    "#         inference_layer = tf.concat([user_combine_layer_flat, movie_combine_layer_flat], 1)  #(?, 200)\n",
    "#         inference = tf.layers.dense(inference_layer, 1,\n",
    "#                                     kernel_initializer=tf.truncated_normal_initializer(stddev=0.01), \n",
    "#                                     kernel_regularizer=tf.nn.l2_loss, name=\"inference\")\n",
    "        #简单的将用户特征和电影特征做矩阵乘法得到一个预测评分\n",
    "#        inference = tf.matmul(user_combine_layer_flat, tf.transpose(movie_combine_layer_flat))\n",
    "        inference = tf.reduce_sum(user_combine_layer_flat * movie_combine_layer_flat, axis=1)\n",
    "        inference = tf.expand_dims(inference, axis=1)\n",
    "\n",
    "    with tf.name_scope(\"loss\"):\n",
    "        # MSE损失，将计算值回归到评分\n",
    "        cost = tf.losses.mean_squared_error(targets, inference )\n",
    "        loss = tf.reduce_mean(cost)\n",
    "    # 优化损失 \n",
    "#     train_op = tf.train.AdamOptimizer(lr).minimize(loss)  #cost\n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    gradients = optimizer.compute_gradients(loss)  #cost\n",
    "    train_op = optimizer.apply_gradients(gradients, global_step=global_step)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'inference/ExpandDims:0' shape=(?, 1) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 取得batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(Xs, ys, batch_size):\n",
    "    for start in range(0, len(Xs), batch_size):\n",
    "        end = min(start + batch_size, len(Xs))\n",
    "        yield Xs[start:end], ys[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\winnie\\Desktop\\movie_recommender-master\\movie_recommender-master\\runs\\1552927358\n",
      "\n",
      "2019-03-18T16:42:43.347917: Epoch   0 Batch    0/3125   train_loss = 32.686\n",
      "2019-03-18T16:42:44.537620: Epoch   0 Batch   20/3125   train_loss = 6.512\n",
      "2019-03-18T16:42:45.760278: Epoch   0 Batch   40/3125   train_loss = 4.406\n",
      "2019-03-18T16:42:47.125410: Epoch   0 Batch   60/3125   train_loss = 3.441\n",
      "2019-03-18T16:42:48.407956: Epoch   0 Batch   80/3125   train_loss = 2.720\n",
      "2019-03-18T16:42:49.823633: Epoch   0 Batch  100/3125   train_loss = 2.677\n",
      "2019-03-18T16:42:51.177748: Epoch   0 Batch  120/3125   train_loss = 2.463\n",
      "2019-03-18T16:42:52.520522: Epoch   0 Batch  140/3125   train_loss = 2.065\n",
      "2019-03-18T16:42:53.760283: Epoch   0 Batch  160/3125   train_loss = 1.791\n",
      "2019-03-18T16:42:55.039432: Epoch   0 Batch  180/3125   train_loss = 2.067\n",
      "2019-03-18T16:42:56.493461: Epoch   0 Batch  200/3125   train_loss = 1.949\n",
      "2019-03-18T16:42:57.728065: Epoch   0 Batch  220/3125   train_loss = 1.921\n",
      "2019-03-18T16:42:58.998411: Epoch   0 Batch  240/3125   train_loss = 1.726\n",
      "2019-03-18T16:43:00.342899: Epoch   0 Batch  260/3125   train_loss = 1.772\n",
      "2019-03-18T16:43:01.746911: Epoch   0 Batch  280/3125   train_loss = 1.879\n",
      "2019-03-18T16:43:03.224845: Epoch   0 Batch  300/3125   train_loss = 1.580\n",
      "2019-03-18T16:43:04.649089: Epoch   0 Batch  320/3125   train_loss = 1.756\n",
      "2019-03-18T16:43:06.044641: Epoch   0 Batch  340/3125   train_loss = 1.573\n",
      "2019-03-18T16:43:07.336480: Epoch   0 Batch  360/3125   train_loss = 1.554\n",
      "2019-03-18T16:43:08.753411: Epoch   0 Batch  380/3125   train_loss = 1.579\n",
      "2019-03-18T16:43:10.136532: Epoch   0 Batch  400/3125   train_loss = 1.313\n",
      "2019-03-18T16:43:11.543983: Epoch   0 Batch  420/3125   train_loss = 1.407\n",
      "2019-03-18T16:43:12.840544: Epoch   0 Batch  440/3125   train_loss = 1.584\n",
      "2019-03-18T16:43:14.256161: Epoch   0 Batch  460/3125   train_loss = 1.504\n",
      "2019-03-18T16:43:15.664580: Epoch   0 Batch  480/3125   train_loss = 1.480\n",
      "2019-03-18T16:43:17.138246: Epoch   0 Batch  500/3125   train_loss = 1.154\n",
      "2019-03-18T16:43:18.634937: Epoch   0 Batch  520/3125   train_loss = 1.454\n",
      "2019-03-18T16:43:20.030798: Epoch   0 Batch  540/3125   train_loss = 1.364\n",
      "2019-03-18T16:43:21.438230: Epoch   0 Batch  560/3125   train_loss = 1.525\n",
      "2019-03-18T16:43:22.858596: Epoch   0 Batch  580/3125   train_loss = 1.729\n",
      "2019-03-18T16:43:24.331026: Epoch   0 Batch  600/3125   train_loss = 1.421\n",
      "2019-03-18T16:43:25.685279: Epoch   0 Batch  620/3125   train_loss = 1.508\n",
      "2019-03-18T16:43:26.931937: Epoch   0 Batch  640/3125   train_loss = 1.433\n",
      "2019-03-18T16:43:28.214433: Epoch   0 Batch  660/3125   train_loss = 1.357\n",
      "2019-03-18T16:43:29.535508: Epoch   0 Batch  680/3125   train_loss = 1.325\n",
      "2019-03-18T16:43:30.836418: Epoch   0 Batch  700/3125   train_loss = 1.411\n",
      "2019-03-18T16:43:32.341632: Epoch   0 Batch  720/3125   train_loss = 1.247\n",
      "2019-03-18T16:43:33.643474: Epoch   0 Batch  740/3125   train_loss = 1.506\n",
      "2019-03-18T16:43:35.035648: Epoch   0 Batch  760/3125   train_loss = 1.416\n",
      "2019-03-18T16:43:36.453221: Epoch   0 Batch  780/3125   train_loss = 1.544\n",
      "2019-03-18T16:43:37.893467: Epoch   0 Batch  800/3125   train_loss = 1.344\n",
      "2019-03-18T16:43:39.254953: Epoch   0 Batch  820/3125   train_loss = 1.201\n",
      "2019-03-18T16:43:40.574713: Epoch   0 Batch  840/3125   train_loss = 1.362\n",
      "2019-03-18T16:43:41.980908: Epoch   0 Batch  860/3125   train_loss = 1.280\n",
      "2019-03-18T16:43:43.393136: Epoch   0 Batch  880/3125   train_loss = 1.312\n",
      "2019-03-18T16:43:44.801044: Epoch   0 Batch  900/3125   train_loss = 1.404\n",
      "2019-03-18T16:43:46.214105: Epoch   0 Batch  920/3125   train_loss = 1.385\n",
      "2019-03-18T16:43:47.925286: Epoch   0 Batch  940/3125   train_loss = 1.427\n",
      "2019-03-18T16:43:49.346325: Epoch   0 Batch  960/3125   train_loss = 1.432\n",
      "2019-03-18T16:43:50.817245: Epoch   0 Batch  980/3125   train_loss = 1.446\n",
      "2019-03-18T16:43:52.365638: Epoch   0 Batch 1000/3125   train_loss = 1.361\n",
      "2019-03-18T16:43:53.904412: Epoch   0 Batch 1020/3125   train_loss = 1.361\n",
      "2019-03-18T16:43:55.541729: Epoch   0 Batch 1040/3125   train_loss = 1.215\n",
      "2019-03-18T16:43:57.279522: Epoch   0 Batch 1060/3125   train_loss = 1.442\n",
      "2019-03-18T16:43:58.903841: Epoch   0 Batch 1080/3125   train_loss = 1.189\n",
      "2019-03-18T16:44:00.503070: Epoch   0 Batch 1100/3125   train_loss = 1.500\n",
      "2019-03-18T16:44:02.038028: Epoch   0 Batch 1120/3125   train_loss = 1.347\n",
      "2019-03-18T16:44:03.538746: Epoch   0 Batch 1140/3125   train_loss = 1.364\n",
      "2019-03-18T16:44:05.101885: Epoch   0 Batch 1160/3125   train_loss = 1.376\n",
      "2019-03-18T16:44:06.512304: Epoch   0 Batch 1180/3125   train_loss = 1.371\n",
      "2019-03-18T16:44:07.897681: Epoch   0 Batch 1200/3125   train_loss = 1.244\n",
      "2019-03-18T16:44:09.258700: Epoch   0 Batch 1220/3125   train_loss = 1.139\n",
      "2019-03-18T16:44:10.676291: Epoch   0 Batch 1240/3125   train_loss = 1.109\n",
      "2019-03-18T16:44:12.024284: Epoch   0 Batch 1260/3125   train_loss = 1.283\n",
      "2019-03-18T16:44:13.351516: Epoch   0 Batch 1280/3125   train_loss = 1.255\n",
      "2019-03-18T16:44:14.705875: Epoch   0 Batch 1300/3125   train_loss = 1.248\n",
      "2019-03-18T16:44:16.034934: Epoch   0 Batch 1320/3125   train_loss = 1.201\n",
      "2019-03-18T16:44:17.415375: Epoch   0 Batch 1340/3125   train_loss = 1.219\n",
      "2019-03-18T16:44:18.741099: Epoch   0 Batch 1360/3125   train_loss = 1.244\n",
      "2019-03-18T16:44:20.115116: Epoch   0 Batch 1380/3125   train_loss = 1.138\n",
      "2019-03-18T16:44:21.380866: Epoch   0 Batch 1400/3125   train_loss = 1.254\n",
      "2019-03-18T16:44:22.715228: Epoch   0 Batch 1420/3125   train_loss = 1.312\n",
      "2019-03-18T16:44:24.003326: Epoch   0 Batch 1440/3125   train_loss = 1.201\n",
      "2019-03-18T16:44:25.322308: Epoch   0 Batch 1460/3125   train_loss = 1.219\n",
      "2019-03-18T16:44:26.762805: Epoch   0 Batch 1480/3125   train_loss = 1.302\n",
      "2019-03-18T16:44:28.118949: Epoch   0 Batch 1500/3125   train_loss = 1.398\n",
      "2019-03-18T16:44:29.530717: Epoch   0 Batch 1520/3125   train_loss = 1.306\n",
      "2019-03-18T16:44:30.956699: Epoch   0 Batch 1540/3125   train_loss = 1.285\n",
      "2019-03-18T16:44:32.491245: Epoch   0 Batch 1560/3125   train_loss = 1.163\n",
      "2019-03-18T16:44:33.810768: Epoch   0 Batch 1580/3125   train_loss = 1.252\n",
      "2019-03-18T16:44:35.256638: Epoch   0 Batch 1600/3125   train_loss = 1.269\n",
      "2019-03-18T16:44:36.672888: Epoch   0 Batch 1620/3125   train_loss = 1.215\n",
      "2019-03-18T16:44:38.170998: Epoch   0 Batch 1640/3125   train_loss = 1.328\n",
      "2019-03-18T16:44:39.661973: Epoch   0 Batch 1660/3125   train_loss = 1.316\n",
      "2019-03-18T16:44:41.090088: Epoch   0 Batch 1680/3125   train_loss = 1.318\n",
      "2019-03-18T16:44:42.444539: Epoch   0 Batch 1700/3125   train_loss = 1.098\n",
      "2019-03-18T16:44:43.711232: Epoch   0 Batch 1720/3125   train_loss = 1.176\n",
      "2019-03-18T16:44:45.146203: Epoch   0 Batch 1740/3125   train_loss = 1.258\n",
      "2019-03-18T16:44:46.534247: Epoch   0 Batch 1760/3125   train_loss = 1.276\n",
      "2019-03-18T16:44:47.901548: Epoch   0 Batch 1780/3125   train_loss = 1.103\n",
      "2019-03-18T16:44:49.341424: Epoch   0 Batch 1800/3125   train_loss = 1.189\n",
      "2019-03-18T16:44:50.673959: Epoch   0 Batch 1820/3125   train_loss = 1.235\n",
      "2019-03-18T16:44:51.944555: Epoch   0 Batch 1840/3125   train_loss = 1.317\n",
      "2019-03-18T16:44:53.246569: Epoch   0 Batch 1860/3125   train_loss = 1.312\n",
      "2019-03-18T16:44:54.668097: Epoch   0 Batch 1880/3125   train_loss = 1.311\n",
      "2019-03-18T16:44:56.050252: Epoch   0 Batch 1900/3125   train_loss = 1.098\n",
      "2019-03-18T16:44:57.424413: Epoch   0 Batch 1920/3125   train_loss = 1.130\n",
      "2019-03-18T16:44:58.865658: Epoch   0 Batch 1940/3125   train_loss = 1.102\n",
      "2019-03-18T16:45:00.265318: Epoch   0 Batch 1960/3125   train_loss = 1.095\n",
      "2019-03-18T16:45:01.518961: Epoch   0 Batch 1980/3125   train_loss = 1.163\n",
      "2019-03-18T16:45:02.819431: Epoch   0 Batch 2000/3125   train_loss = 1.463\n",
      "2019-03-18T16:45:04.078763: Epoch   0 Batch 2020/3125   train_loss = 1.302\n",
      "2019-03-18T16:45:05.335632: Epoch   0 Batch 2040/3125   train_loss = 1.121\n",
      "2019-03-18T16:45:06.609232: Epoch   0 Batch 2060/3125   train_loss = 1.026\n",
      "2019-03-18T16:45:07.839073: Epoch   0 Batch 2080/3125   train_loss = 1.306\n",
      "2019-03-18T16:45:09.123475: Epoch   0 Batch 2100/3125   train_loss = 1.098\n",
      "2019-03-18T16:45:10.465967: Epoch   0 Batch 2120/3125   train_loss = 1.091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18T16:45:11.806770: Epoch   0 Batch 2140/3125   train_loss = 1.193\n",
      "2019-03-18T16:45:13.212141: Epoch   0 Batch 2160/3125   train_loss = 1.140\n",
      "2019-03-18T16:45:14.609257: Epoch   0 Batch 2180/3125   train_loss = 1.248\n",
      "2019-03-18T16:45:15.973512: Epoch   0 Batch 2200/3125   train_loss = 1.148\n",
      "2019-03-18T16:45:17.317879: Epoch   0 Batch 2220/3125   train_loss = 1.139\n",
      "2019-03-18T16:45:18.603676: Epoch   0 Batch 2240/3125   train_loss = 1.085\n",
      "2019-03-18T16:45:20.009649: Epoch   0 Batch 2260/3125   train_loss = 1.130\n",
      "2019-03-18T16:45:21.317946: Epoch   0 Batch 2280/3125   train_loss = 1.262\n",
      "2019-03-18T16:45:22.684529: Epoch   0 Batch 2300/3125   train_loss = 1.270\n",
      "2019-03-18T16:45:24.048231: Epoch   0 Batch 2320/3125   train_loss = 1.333\n",
      "2019-03-18T16:45:25.414352: Epoch   0 Batch 2340/3125   train_loss = 1.274\n",
      "2019-03-18T16:45:26.983608: Epoch   0 Batch 2360/3125   train_loss = 1.142\n",
      "2019-03-18T16:45:28.461720: Epoch   0 Batch 2380/3125   train_loss = 1.193\n",
      "2019-03-18T16:45:29.979530: Epoch   0 Batch 2400/3125   train_loss = 1.302\n",
      "2019-03-18T16:45:31.272067: Epoch   0 Batch 2420/3125   train_loss = 1.176\n",
      "2019-03-18T16:45:32.539400: Epoch   0 Batch 2440/3125   train_loss = 1.297\n",
      "2019-03-18T16:45:34.086297: Epoch   0 Batch 2460/3125   train_loss = 1.071\n",
      "2019-03-18T16:45:35.531713: Epoch   0 Batch 2480/3125   train_loss = 1.251\n",
      "2019-03-18T16:45:36.924878: Epoch   0 Batch 2500/3125   train_loss = 1.279\n",
      "2019-03-18T16:45:38.382860: Epoch   0 Batch 2520/3125   train_loss = 1.070\n",
      "2019-03-18T16:45:39.773896: Epoch   0 Batch 2540/3125   train_loss = 1.149\n",
      "2019-03-18T16:45:41.077128: Epoch   0 Batch 2560/3125   train_loss = 1.023\n",
      "2019-03-18T16:45:42.378712: Epoch   0 Batch 2580/3125   train_loss = 1.125\n",
      "2019-03-18T16:45:43.751466: Epoch   0 Batch 2600/3125   train_loss = 1.167\n",
      "2019-03-18T16:45:45.119400: Epoch   0 Batch 2620/3125   train_loss = 1.103\n",
      "2019-03-18T16:45:46.540239: Epoch   0 Batch 2640/3125   train_loss = 1.114\n",
      "2019-03-18T16:45:47.787501: Epoch   0 Batch 2660/3125   train_loss = 1.223\n",
      "2019-03-18T16:45:49.045814: Epoch   0 Batch 2680/3125   train_loss = 1.058\n",
      "2019-03-18T16:45:50.348409: Epoch   0 Batch 2700/3125   train_loss = 1.216\n",
      "2019-03-18T16:45:51.600950: Epoch   0 Batch 2720/3125   train_loss = 1.161\n",
      "2019-03-18T16:45:52.913886: Epoch   0 Batch 2740/3125   train_loss = 1.154\n",
      "2019-03-18T16:45:54.155443: Epoch   0 Batch 2760/3125   train_loss = 1.245\n",
      "2019-03-18T16:45:55.449921: Epoch   0 Batch 2780/3125   train_loss = 1.135\n",
      "2019-03-18T16:45:56.818192: Epoch   0 Batch 2800/3125   train_loss = 1.420\n",
      "2019-03-18T16:45:58.283742: Epoch   0 Batch 2820/3125   train_loss = 1.450\n",
      "2019-03-18T16:45:59.667998: Epoch   0 Batch 2840/3125   train_loss = 1.153\n",
      "2019-03-18T16:46:01.197026: Epoch   0 Batch 2860/3125   train_loss = 1.131\n",
      "2019-03-18T16:46:02.740509: Epoch   0 Batch 2880/3125   train_loss = 1.168\n",
      "2019-03-18T16:46:04.223423: Epoch   0 Batch 2900/3125   train_loss = 1.174\n",
      "2019-03-18T16:46:05.608656: Epoch   0 Batch 2920/3125   train_loss = 1.218\n",
      "2019-03-18T16:46:07.106651: Epoch   0 Batch 2940/3125   train_loss = 1.130\n",
      "2019-03-18T16:46:08.555691: Epoch   0 Batch 2960/3125   train_loss = 1.181\n",
      "2019-03-18T16:46:09.966978: Epoch   0 Batch 2980/3125   train_loss = 1.193\n",
      "2019-03-18T16:46:11.415354: Epoch   0 Batch 3000/3125   train_loss = 1.096\n",
      "2019-03-18T16:46:12.830050: Epoch   0 Batch 3020/3125   train_loss = 1.248\n",
      "2019-03-18T16:46:14.079688: Epoch   0 Batch 3040/3125   train_loss = 1.123\n",
      "2019-03-18T16:46:15.329665: Epoch   0 Batch 3060/3125   train_loss = 1.214\n",
      "2019-03-18T16:46:16.578848: Epoch   0 Batch 3080/3125   train_loss = 1.316\n",
      "2019-03-18T16:46:17.935399: Epoch   0 Batch 3100/3125   train_loss = 1.193\n",
      "2019-03-18T16:46:19.218042: Epoch   0 Batch 3120/3125   train_loss = 1.059\n",
      "2019-03-18T16:46:19.717473: Epoch   0 Batch    0/781   test_loss = 1.028\n",
      "2019-03-18T16:46:20.093403: Epoch   0 Batch   20/781   test_loss = 1.175\n",
      "2019-03-18T16:46:20.456052: Epoch   0 Batch   40/781   test_loss = 1.143\n",
      "2019-03-18T16:46:20.823624: Epoch   0 Batch   60/781   test_loss = 1.334\n",
      "2019-03-18T16:46:21.200872: Epoch   0 Batch   80/781   test_loss = 1.377\n",
      "2019-03-18T16:46:21.558727: Epoch   0 Batch  100/781   test_loss = 1.370\n",
      "2019-03-18T16:46:21.939611: Epoch   0 Batch  120/781   test_loss = 1.311\n",
      "2019-03-18T16:46:22.334309: Epoch   0 Batch  140/781   test_loss = 1.256\n",
      "2019-03-18T16:46:22.706732: Epoch   0 Batch  160/781   test_loss = 1.389\n",
      "2019-03-18T16:46:23.132168: Epoch   0 Batch  180/781   test_loss = 1.218\n",
      "2019-03-18T16:46:23.504601: Epoch   0 Batch  200/781   test_loss = 1.212\n",
      "2019-03-18T16:46:23.874179: Epoch   0 Batch  220/781   test_loss = 1.012\n",
      "2019-03-18T16:46:24.288077: Epoch   0 Batch  240/781   test_loss = 1.176\n",
      "2019-03-18T16:46:24.688997: Epoch   0 Batch  260/781   test_loss = 1.261\n",
      "2019-03-18T16:46:25.042725: Epoch   0 Batch  280/781   test_loss = 1.486\n",
      "2019-03-18T16:46:25.395801: Epoch   0 Batch  300/781   test_loss = 1.219\n",
      "2019-03-18T16:46:25.795371: Epoch   0 Batch  320/781   test_loss = 1.305\n",
      "2019-03-18T16:46:26.205363: Epoch   0 Batch  340/781   test_loss = 0.908\n",
      "2019-03-18T16:46:26.830324: Epoch   0 Batch  360/781   test_loss = 1.305\n",
      "2019-03-18T16:46:27.438344: Epoch   0 Batch  380/781   test_loss = 1.152\n",
      "2019-03-18T16:46:28.007331: Epoch   0 Batch  400/781   test_loss = 1.149\n",
      "2019-03-18T16:46:28.604320: Epoch   0 Batch  420/781   test_loss = 1.085\n",
      "2019-03-18T16:46:29.157312: Epoch   0 Batch  440/781   test_loss = 1.273\n",
      "2019-03-18T16:46:29.655303: Epoch   0 Batch  460/781   test_loss = 1.131\n",
      "2019-03-18T16:46:30.154295: Epoch   0 Batch  480/781   test_loss = 1.136\n",
      "2019-03-18T16:46:30.601286: Epoch   0 Batch  500/781   test_loss = 1.010\n",
      "2019-03-18T16:46:31.019404: Epoch   0 Batch  520/781   test_loss = 1.195\n",
      "2019-03-18T16:46:31.432030: Epoch   0 Batch  540/781   test_loss = 1.023\n",
      "2019-03-18T16:46:31.843833: Epoch   0 Batch  560/781   test_loss = 1.323\n",
      "2019-03-18T16:46:32.295618: Epoch   0 Batch  580/781   test_loss = 1.173\n",
      "2019-03-18T16:46:32.686967: Epoch   0 Batch  600/781   test_loss = 1.173\n",
      "2019-03-18T16:46:33.095529: Epoch   0 Batch  620/781   test_loss = 1.182\n",
      "2019-03-18T16:46:33.486405: Epoch   0 Batch  640/781   test_loss = 1.264\n",
      "2019-03-18T16:46:33.861644: Epoch   0 Batch  660/781   test_loss = 1.167\n",
      "2019-03-18T16:46:34.239089: Epoch   0 Batch  680/781   test_loss = 1.451\n",
      "2019-03-18T16:46:34.688986: Epoch   0 Batch  700/781   test_loss = 1.164\n",
      "2019-03-18T16:46:35.093500: Epoch   0 Batch  720/781   test_loss = 1.356\n",
      "2019-03-18T16:46:35.461196: Epoch   0 Batch  740/781   test_loss = 1.229\n",
      "2019-03-18T16:46:35.867377: Epoch   0 Batch  760/781   test_loss = 1.136\n",
      "2019-03-18T16:46:36.267505: Epoch   0 Batch  780/781   test_loss = 1.233\n",
      "2019-03-18T16:46:38.325453: Epoch   1 Batch   15/3125   train_loss = 1.260\n",
      "2019-03-18T16:46:39.727626: Epoch   1 Batch   35/3125   train_loss = 1.124\n",
      "2019-03-18T16:46:41.002191: Epoch   1 Batch   55/3125   train_loss = 1.309\n",
      "2019-03-18T16:46:42.241455: Epoch   1 Batch   75/3125   train_loss = 1.202\n",
      "2019-03-18T16:46:43.478435: Epoch   1 Batch   95/3125   train_loss = 1.029\n",
      "2019-03-18T16:46:44.717333: Epoch   1 Batch  115/3125   train_loss = 1.205\n",
      "2019-03-18T16:46:46.004452: Epoch   1 Batch  135/3125   train_loss = 1.034\n",
      "2019-03-18T16:46:47.352749: Epoch   1 Batch  155/3125   train_loss = 1.100\n",
      "2019-03-18T16:46:48.876949: Epoch   1 Batch  175/3125   train_loss = 1.097\n",
      "2019-03-18T16:46:50.496078: Epoch   1 Batch  195/3125   train_loss = 1.275\n",
      "2019-03-18T16:46:52.167230: Epoch   1 Batch  215/3125   train_loss = 1.139\n",
      "2019-03-18T16:46:53.646316: Epoch   1 Batch  235/3125   train_loss = 1.059\n",
      "2019-03-18T16:46:55.045291: Epoch   1 Batch  255/3125   train_loss = 1.272\n",
      "2019-03-18T16:46:56.546264: Epoch   1 Batch  275/3125   train_loss = 1.082\n",
      "2019-03-18T16:46:58.070208: Epoch   1 Batch  295/3125   train_loss = 1.097\n",
      "2019-03-18T16:46:59.731207: Epoch   1 Batch  315/3125   train_loss = 1.078\n",
      "2019-03-18T16:47:01.369179: Epoch   1 Batch  335/3125   train_loss = 1.109\n",
      "2019-03-18T16:47:02.976173: Epoch   1 Batch  355/3125   train_loss = 1.203\n",
      "2019-03-18T16:47:04.340922: Epoch   1 Batch  375/3125   train_loss = 1.184\n",
      "2019-03-18T16:47:05.644384: Epoch   1 Batch  395/3125   train_loss = 1.058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18T16:47:06.915538: Epoch   1 Batch  415/3125   train_loss = 1.272\n",
      "2019-03-18T16:47:08.193975: Epoch   1 Batch  435/3125   train_loss = 1.227\n",
      "2019-03-18T16:47:09.446403: Epoch   1 Batch  455/3125   train_loss = 1.060\n",
      "2019-03-18T16:47:10.753340: Epoch   1 Batch  475/3125   train_loss = 1.185\n",
      "2019-03-18T16:47:11.975439: Epoch   1 Batch  495/3125   train_loss = 1.084\n",
      "2019-03-18T16:47:13.347064: Epoch   1 Batch  515/3125   train_loss = 1.208\n",
      "2019-03-18T16:47:14.565369: Epoch   1 Batch  535/3125   train_loss = 1.158\n",
      "2019-03-18T16:47:15.869030: Epoch   1 Batch  555/3125   train_loss = 1.248\n",
      "2019-03-18T16:47:17.156892: Epoch   1 Batch  575/3125   train_loss = 1.167\n",
      "2019-03-18T16:47:18.427767: Epoch   1 Batch  595/3125   train_loss = 1.281\n",
      "2019-03-18T16:47:19.646134: Epoch   1 Batch  615/3125   train_loss = 1.066\n",
      "2019-03-18T16:47:20.885356: Epoch   1 Batch  635/3125   train_loss = 1.157\n",
      "2019-03-18T16:47:22.119015: Epoch   1 Batch  655/3125   train_loss = 1.019\n",
      "2019-03-18T16:47:23.339157: Epoch   1 Batch  675/3125   train_loss = 0.912\n",
      "2019-03-18T16:47:24.638415: Epoch   1 Batch  695/3125   train_loss = 1.085\n",
      "2019-03-18T16:47:25.919900: Epoch   1 Batch  715/3125   train_loss = 1.156\n",
      "2019-03-18T16:47:27.155632: Epoch   1 Batch  735/3125   train_loss = 1.016\n",
      "2019-03-18T16:47:28.384879: Epoch   1 Batch  755/3125   train_loss = 1.117\n",
      "2019-03-18T16:47:29.592760: Epoch   1 Batch  775/3125   train_loss = 1.021\n",
      "2019-03-18T16:47:30.793802: Epoch   1 Batch  795/3125   train_loss = 1.240\n",
      "2019-03-18T16:47:32.023277: Epoch   1 Batch  815/3125   train_loss = 1.205\n",
      "2019-03-18T16:47:33.239320: Epoch   1 Batch  835/3125   train_loss = 1.117\n",
      "2019-03-18T16:47:34.457639: Epoch   1 Batch  855/3125   train_loss = 1.276\n",
      "2019-03-18T16:47:35.749823: Epoch   1 Batch  875/3125   train_loss = 1.193\n",
      "2019-03-18T16:47:36.974117: Epoch   1 Batch  895/3125   train_loss = 1.024\n",
      "2019-03-18T16:47:38.184755: Epoch   1 Batch  915/3125   train_loss = 1.113\n",
      "2019-03-18T16:47:39.419746: Epoch   1 Batch  935/3125   train_loss = 1.254\n",
      "2019-03-18T16:47:40.696917: Epoch   1 Batch  955/3125   train_loss = 1.194\n",
      "2019-03-18T16:47:41.972118: Epoch   1 Batch  975/3125   train_loss = 1.079\n",
      "2019-03-18T16:47:43.356263: Epoch   1 Batch  995/3125   train_loss = 0.873\n",
      "2019-03-18T16:47:44.567971: Epoch   1 Batch 1015/3125   train_loss = 1.116\n",
      "2019-03-18T16:47:45.929211: Epoch   1 Batch 1035/3125   train_loss = 1.090\n",
      "2019-03-18T16:47:47.301464: Epoch   1 Batch 1055/3125   train_loss = 1.081\n",
      "2019-03-18T16:47:48.912480: Epoch   1 Batch 1075/3125   train_loss = 1.060\n",
      "2019-03-18T16:47:50.564123: Epoch   1 Batch 1095/3125   train_loss = 1.059\n",
      "2019-03-18T16:47:51.943273: Epoch   1 Batch 1115/3125   train_loss = 1.129\n",
      "2019-03-18T16:47:53.485144: Epoch   1 Batch 1135/3125   train_loss = 1.030\n",
      "2019-03-18T16:47:54.844210: Epoch   1 Batch 1155/3125   train_loss = 1.142\n",
      "2019-03-18T16:47:56.320325: Epoch   1 Batch 1175/3125   train_loss = 1.149\n",
      "2019-03-18T16:47:57.796237: Epoch   1 Batch 1195/3125   train_loss = 1.319\n",
      "2019-03-18T16:47:59.198100: Epoch   1 Batch 1215/3125   train_loss = 0.973\n",
      "2019-03-18T16:48:00.575006: Epoch   1 Batch 1235/3125   train_loss = 1.087\n",
      "2019-03-18T16:48:01.948554: Epoch   1 Batch 1255/3125   train_loss = 1.011\n",
      "2019-03-18T16:48:03.255187: Epoch   1 Batch 1275/3125   train_loss = 1.023\n",
      "2019-03-18T16:48:04.693218: Epoch   1 Batch 1295/3125   train_loss = 1.055\n",
      "2019-03-18T16:48:06.106569: Epoch   1 Batch 1315/3125   train_loss = 1.214\n",
      "2019-03-18T16:48:07.382878: Epoch   1 Batch 1335/3125   train_loss = 1.061\n",
      "2019-03-18T16:48:08.764508: Epoch   1 Batch 1355/3125   train_loss = 1.029\n",
      "2019-03-18T16:48:10.021574: Epoch   1 Batch 1375/3125   train_loss = 1.132\n",
      "2019-03-18T16:48:11.309818: Epoch   1 Batch 1395/3125   train_loss = 1.083\n",
      "2019-03-18T16:48:12.745225: Epoch   1 Batch 1415/3125   train_loss = 1.168\n",
      "2019-03-18T16:48:14.121001: Epoch   1 Batch 1435/3125   train_loss = 1.224\n",
      "2019-03-18T16:48:15.543543: Epoch   1 Batch 1455/3125   train_loss = 1.270\n",
      "2019-03-18T16:48:16.940713: Epoch   1 Batch 1475/3125   train_loss = 1.085\n",
      "2019-03-18T16:48:18.358826: Epoch   1 Batch 1495/3125   train_loss = 1.044\n",
      "2019-03-18T16:48:19.718184: Epoch   1 Batch 1515/3125   train_loss = 0.988\n",
      "2019-03-18T16:48:21.004526: Epoch   1 Batch 1535/3125   train_loss = 0.907\n",
      "2019-03-18T16:48:22.232033: Epoch   1 Batch 1555/3125   train_loss = 1.091\n",
      "2019-03-18T16:48:23.568167: Epoch   1 Batch 1575/3125   train_loss = 1.049\n",
      "2019-03-18T16:48:24.819519: Epoch   1 Batch 1595/3125   train_loss = 1.130\n",
      "2019-03-18T16:48:26.174796: Epoch   1 Batch 1615/3125   train_loss = 1.080\n",
      "2019-03-18T16:48:27.585114: Epoch   1 Batch 1635/3125   train_loss = 1.116\n",
      "2019-03-18T16:48:29.044916: Epoch   1 Batch 1655/3125   train_loss = 1.116\n",
      "2019-03-18T16:48:30.432111: Epoch   1 Batch 1675/3125   train_loss = 1.025\n",
      "2019-03-18T16:48:31.834382: Epoch   1 Batch 1695/3125   train_loss = 1.017\n",
      "2019-03-18T16:48:33.159777: Epoch   1 Batch 1715/3125   train_loss = 0.908\n",
      "2019-03-18T16:48:34.554873: Epoch   1 Batch 1735/3125   train_loss = 1.205\n",
      "2019-03-18T16:48:35.816220: Epoch   1 Batch 1755/3125   train_loss = 1.005\n",
      "2019-03-18T16:48:37.081598: Epoch   1 Batch 1775/3125   train_loss = 1.083\n",
      "2019-03-18T16:48:38.364379: Epoch   1 Batch 1795/3125   train_loss = 1.107\n",
      "2019-03-18T16:48:39.746086: Epoch   1 Batch 1815/3125   train_loss = 1.043\n",
      "2019-03-18T16:48:40.992722: Epoch   1 Batch 1835/3125   train_loss = 1.166\n",
      "2019-03-18T16:48:42.259059: Epoch   1 Batch 1855/3125   train_loss = 0.927\n",
      "2019-03-18T16:48:43.504089: Epoch   1 Batch 1875/3125   train_loss = 1.136\n",
      "2019-03-18T16:48:44.745459: Epoch   1 Batch 1895/3125   train_loss = 1.028\n",
      "2019-03-18T16:48:46.041824: Epoch   1 Batch 1915/3125   train_loss = 0.969\n",
      "2019-03-18T16:48:47.305534: Epoch   1 Batch 1935/3125   train_loss = 1.031\n",
      "2019-03-18T16:48:48.572658: Epoch   1 Batch 1955/3125   train_loss = 1.016\n",
      "2019-03-18T16:48:49.873973: Epoch   1 Batch 1975/3125   train_loss = 1.087\n",
      "2019-03-18T16:48:51.173327: Epoch   1 Batch 1995/3125   train_loss = 1.144\n",
      "2019-03-18T16:48:52.475235: Epoch   1 Batch 2015/3125   train_loss = 1.173\n",
      "2019-03-18T16:48:53.724284: Epoch   1 Batch 2035/3125   train_loss = 1.182\n",
      "2019-03-18T16:48:55.020711: Epoch   1 Batch 2055/3125   train_loss = 0.996\n",
      "2019-03-18T16:48:56.315941: Epoch   1 Batch 2075/3125   train_loss = 1.153\n",
      "2019-03-18T16:48:57.688228: Epoch   1 Batch 2095/3125   train_loss = 0.970\n",
      "2019-03-18T16:48:59.251522: Epoch   1 Batch 2115/3125   train_loss = 1.145\n",
      "2019-03-18T16:49:00.565680: Epoch   1 Batch 2135/3125   train_loss = 1.043\n",
      "2019-03-18T16:49:02.102661: Epoch   1 Batch 2155/3125   train_loss = 1.037\n",
      "2019-03-18T16:49:03.506187: Epoch   1 Batch 2175/3125   train_loss = 1.087\n",
      "2019-03-18T16:49:05.016805: Epoch   1 Batch 2195/3125   train_loss = 1.074\n",
      "2019-03-18T16:49:06.491596: Epoch   1 Batch 2215/3125   train_loss = 1.020\n",
      "2019-03-18T16:49:07.909144: Epoch   1 Batch 2235/3125   train_loss = 1.152\n",
      "2019-03-18T16:49:09.300224: Epoch   1 Batch 2255/3125   train_loss = 1.174\n",
      "2019-03-18T16:49:10.697626: Epoch   1 Batch 2275/3125   train_loss = 0.925\n",
      "2019-03-18T16:49:12.094964: Epoch   1 Batch 2295/3125   train_loss = 1.284\n",
      "2019-03-18T16:49:13.444369: Epoch   1 Batch 2315/3125   train_loss = 1.200\n",
      "2019-03-18T16:49:14.856850: Epoch   1 Batch 2335/3125   train_loss = 1.125\n",
      "2019-03-18T16:49:16.208337: Epoch   1 Batch 2355/3125   train_loss = 1.084\n",
      "2019-03-18T16:49:17.673030: Epoch   1 Batch 2375/3125   train_loss = 1.246\n",
      "2019-03-18T16:49:19.053240: Epoch   1 Batch 2395/3125   train_loss = 1.056\n",
      "2019-03-18T16:49:20.333689: Epoch   1 Batch 2415/3125   train_loss = 1.122\n",
      "2019-03-18T16:49:21.584155: Epoch   1 Batch 2435/3125   train_loss = 0.995\n",
      "2019-03-18T16:49:22.913578: Epoch   1 Batch 2455/3125   train_loss = 1.139\n",
      "2019-03-18T16:49:24.240031: Epoch   1 Batch 2475/3125   train_loss = 0.971\n",
      "2019-03-18T16:49:25.553382: Epoch   1 Batch 2495/3125   train_loss = 1.049\n",
      "2019-03-18T16:49:26.848186: Epoch   1 Batch 2515/3125   train_loss = 1.088\n",
      "2019-03-18T16:49:28.207776: Epoch   1 Batch 2535/3125   train_loss = 1.067\n",
      "2019-03-18T16:49:29.543349: Epoch   1 Batch 2555/3125   train_loss = 0.904\n",
      "2019-03-18T16:49:30.864662: Epoch   1 Batch 2575/3125   train_loss = 1.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18T16:49:32.242331: Epoch   1 Batch 2595/3125   train_loss = 1.036\n",
      "2019-03-18T16:49:33.680596: Epoch   1 Batch 2615/3125   train_loss = 1.122\n",
      "2019-03-18T16:49:35.055964: Epoch   1 Batch 2635/3125   train_loss = 0.956\n",
      "2019-03-18T16:49:36.288766: Epoch   1 Batch 2655/3125   train_loss = 1.062\n",
      "2019-03-18T16:49:37.538637: Epoch   1 Batch 2675/3125   train_loss = 0.968\n",
      "2019-03-18T16:49:38.921278: Epoch   1 Batch 2695/3125   train_loss = 1.024\n",
      "2019-03-18T16:49:40.285501: Epoch   1 Batch 2715/3125   train_loss = 1.032\n",
      "2019-03-18T16:49:41.658271: Epoch   1 Batch 2735/3125   train_loss = 0.879\n",
      "2019-03-18T16:49:43.046959: Epoch   1 Batch 2755/3125   train_loss = 1.111\n",
      "2019-03-18T16:49:44.367646: Epoch   1 Batch 2775/3125   train_loss = 1.132\n",
      "2019-03-18T16:49:45.663528: Epoch   1 Batch 2795/3125   train_loss = 1.043\n",
      "2019-03-18T16:49:47.023196: Epoch   1 Batch 2815/3125   train_loss = 1.036\n",
      "2019-03-18T16:49:48.312161: Epoch   1 Batch 2835/3125   train_loss = 1.068\n",
      "2019-03-18T16:49:49.678254: Epoch   1 Batch 2855/3125   train_loss = 1.073\n",
      "2019-03-18T16:49:51.126212: Epoch   1 Batch 2875/3125   train_loss = 0.982\n",
      "2019-03-18T16:49:52.599868: Epoch   1 Batch 2895/3125   train_loss = 1.091\n",
      "2019-03-18T16:49:53.857674: Epoch   1 Batch 2915/3125   train_loss = 0.970\n",
      "2019-03-18T16:49:55.179882: Epoch   1 Batch 2935/3125   train_loss = 1.092\n",
      "2019-03-18T16:49:56.551845: Epoch   1 Batch 2955/3125   train_loss = 1.087\n",
      "2019-03-18T16:49:57.899645: Epoch   1 Batch 2975/3125   train_loss = 1.073\n",
      "2019-03-18T16:49:59.181631: Epoch   1 Batch 2995/3125   train_loss = 0.998\n",
      "2019-03-18T16:50:00.558200: Epoch   1 Batch 3015/3125   train_loss = 0.990\n",
      "2019-03-18T16:50:02.119876: Epoch   1 Batch 3035/3125   train_loss = 1.075\n",
      "2019-03-18T16:50:03.582389: Epoch   1 Batch 3055/3125   train_loss = 1.108\n",
      "2019-03-18T16:50:05.054584: Epoch   1 Batch 3075/3125   train_loss = 1.029\n",
      "2019-03-18T16:50:06.426400: Epoch   1 Batch 3095/3125   train_loss = 1.019\n",
      "2019-03-18T16:50:07.751839: Epoch   1 Batch 3115/3125   train_loss = 0.897\n",
      "2019-03-18T16:50:08.721226: Epoch   1 Batch   19/781   test_loss = 1.037\n",
      "2019-03-18T16:50:09.121281: Epoch   1 Batch   39/781   test_loss = 0.896\n",
      "2019-03-18T16:50:09.517173: Epoch   1 Batch   59/781   test_loss = 0.906\n",
      "2019-03-18T16:50:09.883202: Epoch   1 Batch   79/781   test_loss = 1.016\n",
      "2019-03-18T16:50:10.259232: Epoch   1 Batch   99/781   test_loss = 1.033\n",
      "2019-03-18T16:50:10.648444: Epoch   1 Batch  119/781   test_loss = 0.967\n",
      "2019-03-18T16:50:11.078448: Epoch   1 Batch  139/781   test_loss = 1.082\n",
      "2019-03-18T16:50:11.469377: Epoch   1 Batch  159/781   test_loss = 1.069\n",
      "2019-03-18T16:50:11.882001: Epoch   1 Batch  179/781   test_loss = 0.976\n",
      "2019-03-18T16:50:12.341233: Epoch   1 Batch  199/781   test_loss = 0.925\n",
      "2019-03-18T16:50:12.792995: Epoch   1 Batch  219/781   test_loss = 1.091\n",
      "2019-03-18T16:50:13.155605: Epoch   1 Batch  239/781   test_loss = 1.245\n",
      "2019-03-18T16:50:13.530537: Epoch   1 Batch  259/781   test_loss = 0.978\n",
      "2019-03-18T16:50:13.905534: Epoch   1 Batch  279/781   test_loss = 1.138\n",
      "2019-03-18T16:50:14.276571: Epoch   1 Batch  299/781   test_loss = 1.233\n",
      "2019-03-18T16:50:14.643577: Epoch   1 Batch  319/781   test_loss = 0.994\n",
      "2019-03-18T16:50:15.002949: Epoch   1 Batch  339/781   test_loss = 0.915\n",
      "2019-03-18T16:50:15.386181: Epoch   1 Batch  359/781   test_loss = 0.969\n",
      "2019-03-18T16:50:15.776804: Epoch   1 Batch  379/781   test_loss = 1.091\n",
      "2019-03-18T16:50:16.143844: Epoch   1 Batch  399/781   test_loss = 0.900\n",
      "2019-03-18T16:50:16.518840: Epoch   1 Batch  419/781   test_loss = 0.972\n",
      "2019-03-18T16:50:16.898175: Epoch   1 Batch  439/781   test_loss = 1.073\n",
      "2019-03-18T16:50:17.277389: Epoch   1 Batch  459/781   test_loss = 1.092\n",
      "2019-03-18T16:50:17.646137: Epoch   1 Batch  479/781   test_loss = 1.043\n",
      "2019-03-18T16:50:18.005579: Epoch   1 Batch  499/781   test_loss = 1.002\n",
      "2019-03-18T16:50:18.374596: Epoch   1 Batch  519/781   test_loss = 1.110\n",
      "2019-03-18T16:50:18.755928: Epoch   1 Batch  539/781   test_loss = 0.881\n",
      "2019-03-18T16:50:19.110018: Epoch   1 Batch  559/781   test_loss = 1.183\n",
      "2019-03-18T16:50:19.490810: Epoch   1 Batch  579/781   test_loss = 1.024\n",
      "2019-03-18T16:50:19.850249: Epoch   1 Batch  599/781   test_loss = 1.043\n",
      "2019-03-18T16:50:20.230452: Epoch   1 Batch  619/781   test_loss = 1.167\n",
      "2019-03-18T16:50:20.584618: Epoch   1 Batch  639/781   test_loss = 0.893\n",
      "2019-03-18T16:50:20.967089: Epoch   1 Batch  659/781   test_loss = 1.153\n",
      "2019-03-18T16:50:21.350232: Epoch   1 Batch  679/781   test_loss = 1.207\n",
      "2019-03-18T16:50:21.735012: Epoch   1 Batch  699/781   test_loss = 0.873\n",
      "2019-03-18T16:50:22.112305: Epoch   1 Batch  719/781   test_loss = 1.015\n",
      "2019-03-18T16:50:22.503644: Epoch   1 Batch  739/781   test_loss = 0.978\n",
      "2019-03-18T16:50:22.896635: Epoch   1 Batch  759/781   test_loss = 0.953\n",
      "2019-03-18T16:50:23.297628: Epoch   1 Batch  779/781   test_loss = 0.796\n",
      "2019-03-18T16:50:25.458589: Epoch   2 Batch   10/3125   train_loss = 0.961\n",
      "2019-03-18T16:50:26.877565: Epoch   2 Batch   30/3125   train_loss = 1.087\n",
      "2019-03-18T16:50:28.378538: Epoch   2 Batch   50/3125   train_loss = 1.105\n",
      "2019-03-18T16:50:29.805387: Epoch   2 Batch   70/3125   train_loss = 1.054\n",
      "2019-03-18T16:50:31.111385: Epoch   2 Batch   90/3125   train_loss = 1.068\n",
      "2019-03-18T16:50:32.407059: Epoch   2 Batch  110/3125   train_loss = 0.979\n",
      "2019-03-18T16:50:33.846535: Epoch   2 Batch  130/3125   train_loss = 0.983\n",
      "2019-03-18T16:50:35.332802: Epoch   2 Batch  150/3125   train_loss = 1.127\n",
      "2019-03-18T16:50:36.647674: Epoch   2 Batch  170/3125   train_loss = 1.001\n",
      "2019-03-18T16:50:37.889555: Epoch   2 Batch  190/3125   train_loss = 1.143\n",
      "2019-03-18T16:50:39.205071: Epoch   2 Batch  210/3125   train_loss = 0.944\n",
      "2019-03-18T16:50:40.629218: Epoch   2 Batch  230/3125   train_loss = 1.069\n",
      "2019-03-18T16:50:42.097420: Epoch   2 Batch  250/3125   train_loss = 0.960\n",
      "2019-03-18T16:50:43.455694: Epoch   2 Batch  270/3125   train_loss = 0.832\n",
      "2019-03-18T16:50:44.840632: Epoch   2 Batch  290/3125   train_loss = 1.087\n",
      "2019-03-18T16:50:46.116743: Epoch   2 Batch  310/3125   train_loss = 0.995\n",
      "2019-03-18T16:50:47.371641: Epoch   2 Batch  330/3125   train_loss = 1.028\n",
      "2019-03-18T16:50:48.642278: Epoch   2 Batch  350/3125   train_loss = 0.942\n",
      "2019-03-18T16:50:49.910965: Epoch   2 Batch  370/3125   train_loss = 1.154\n",
      "2019-03-18T16:50:51.177083: Epoch   2 Batch  390/3125   train_loss = 1.144\n",
      "2019-03-18T16:50:52.417961: Epoch   2 Batch  410/3125   train_loss = 0.910\n",
      "2019-03-18T16:50:53.646644: Epoch   2 Batch  430/3125   train_loss = 1.171\n",
      "2019-03-18T16:50:54.937384: Epoch   2 Batch  450/3125   train_loss = 0.947\n",
      "2019-03-18T16:50:56.381075: Epoch   2 Batch  470/3125   train_loss = 0.934\n",
      "2019-03-18T16:50:57.747262: Epoch   2 Batch  490/3125   train_loss = 1.027\n",
      "2019-03-18T16:50:59.068903: Epoch   2 Batch  510/3125   train_loss = 1.090\n",
      "2019-03-18T16:51:00.422051: Epoch   2 Batch  530/3125   train_loss = 1.021\n",
      "2019-03-18T16:51:01.745047: Epoch   2 Batch  550/3125   train_loss = 0.959\n",
      "2019-03-18T16:51:03.080882: Epoch   2 Batch  570/3125   train_loss = 1.107\n",
      "2019-03-18T16:51:04.391098: Epoch   2 Batch  590/3125   train_loss = 1.064\n",
      "2019-03-18T16:51:05.697004: Epoch   2 Batch  610/3125   train_loss = 0.997\n",
      "2019-03-18T16:51:06.953295: Epoch   2 Batch  630/3125   train_loss = 1.078\n",
      "2019-03-18T16:51:08.185816: Epoch   2 Batch  650/3125   train_loss = 1.052\n",
      "2019-03-18T16:51:09.434918: Epoch   2 Batch  670/3125   train_loss = 0.950\n",
      "2019-03-18T16:51:10.719925: Epoch   2 Batch  690/3125   train_loss = 0.974\n",
      "2019-03-18T16:51:12.038296: Epoch   2 Batch  710/3125   train_loss = 0.954\n",
      "2019-03-18T16:51:13.533190: Epoch   2 Batch  730/3125   train_loss = 0.874\n",
      "2019-03-18T16:51:14.954804: Epoch   2 Batch  750/3125   train_loss = 0.954\n",
      "2019-03-18T16:51:16.257952: Epoch   2 Batch  770/3125   train_loss = 0.892\n",
      "2019-03-18T16:51:17.593220: Epoch   2 Batch  790/3125   train_loss = 0.950\n",
      "2019-03-18T16:51:18.849112: Epoch   2 Batch  810/3125   train_loss = 0.939\n",
      "2019-03-18T16:51:20.100149: Epoch   2 Batch  830/3125   train_loss = 0.813\n",
      "2019-03-18T16:51:21.324252: Epoch   2 Batch  850/3125   train_loss = 0.990\n",
      "2019-03-18T16:51:22.558038: Epoch   2 Batch  870/3125   train_loss = 0.919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-18T16:51:23.891074: Epoch   2 Batch  890/3125   train_loss = 0.969\n",
      "2019-03-18T16:51:25.167860: Epoch   2 Batch  910/3125   train_loss = 1.018\n",
      "2019-03-18T16:51:26.495489: Epoch   2 Batch  930/3125   train_loss = 1.011\n",
      "2019-03-18T16:51:27.771644: Epoch   2 Batch  950/3125   train_loss = 0.973\n",
      "2019-03-18T16:51:29.053571: Epoch   2 Batch  970/3125   train_loss = 1.051\n",
      "2019-03-18T16:51:30.388614: Epoch   2 Batch  990/3125   train_loss = 0.854\n",
      "2019-03-18T16:51:31.713863: Epoch   2 Batch 1010/3125   train_loss = 1.115\n",
      "2019-03-18T16:51:32.986609: Epoch   2 Batch 1030/3125   train_loss = 0.979\n",
      "2019-03-18T16:51:34.239070: Epoch   2 Batch 1050/3125   train_loss = 0.974\n",
      "2019-03-18T16:51:35.501695: Epoch   2 Batch 1070/3125   train_loss = 1.026\n",
      "2019-03-18T16:51:36.782304: Epoch   2 Batch 1090/3125   train_loss = 1.127\n",
      "2019-03-18T16:51:38.022482: Epoch   2 Batch 1110/3125   train_loss = 1.027\n",
      "2019-03-18T16:51:39.286655: Epoch   2 Batch 1130/3125   train_loss = 0.989\n",
      "2019-03-18T16:51:40.560120: Epoch   2 Batch 1150/3125   train_loss = 0.978\n",
      "2019-03-18T16:51:41.803294: Epoch   2 Batch 1170/3125   train_loss = 1.044\n",
      "2019-03-18T16:51:43.176434: Epoch   2 Batch 1190/3125   train_loss = 1.032\n",
      "2019-03-18T16:51:44.545795: Epoch   2 Batch 1210/3125   train_loss = 0.877\n",
      "2019-03-18T16:51:45.889796: Epoch   2 Batch 1230/3125   train_loss = 0.887\n",
      "2019-03-18T16:51:47.260220: Epoch   2 Batch 1250/3125   train_loss = 1.023\n",
      "2019-03-18T16:51:48.569118: Epoch   2 Batch 1270/3125   train_loss = 1.037\n",
      "2019-03-18T16:51:49.844575: Epoch   2 Batch 1290/3125   train_loss = 0.970\n",
      "2019-03-18T16:51:51.147641: Epoch   2 Batch 1310/3125   train_loss = 1.036\n",
      "2019-03-18T16:51:52.593097: Epoch   2 Batch 1330/3125   train_loss = 1.117\n",
      "2019-03-18T16:51:54.071070: Epoch   2 Batch 1350/3125   train_loss = 0.942\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "losses = {'train':[], 'test':[]}\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    \n",
    "    #搜集数据给tensorBoard用\n",
    "    # Keep track of gradient values and sparsity\n",
    "    grad_summaries = []\n",
    "    for g, v in gradients:\n",
    "        if g is not None:\n",
    "            grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name.replace(':', '_')), g)\n",
    "            sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name.replace(':', '_')), tf.nn.zero_fraction(g))\n",
    "            grad_summaries.append(grad_hist_summary)\n",
    "            grad_summaries.append(sparsity_summary)\n",
    "    grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "        \n",
    "    # Output directory for models and summaries\n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "     \n",
    "    # Summaries for loss and accuracy\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "    # Train Summaries\n",
    "    train_summary_op = tf.summary.merge([loss_summary, grad_summaries_merged])\n",
    "    train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "    train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "    # Inference summaries\n",
    "    inference_summary_op = tf.summary.merge([loss_summary])\n",
    "    inference_summary_dir = os.path.join(out_dir, \"summaries\", \"inference\")\n",
    "    inference_summary_writer = tf.summary.FileWriter(inference_summary_dir, sess.graph)\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    for epoch_i in range(num_epochs):\n",
    "        \n",
    "        #将数据集分成训练集和测试集，随机种子不固定\n",
    "        train_X,test_X, train_y, test_y = train_test_split(features,  \n",
    "                                                           targets_values,  \n",
    "                                                           test_size = 0.2,  \n",
    "                                                           random_state = 0)  \n",
    "        \n",
    "        train_batches = get_batches(train_X, train_y, batch_size)\n",
    "        test_batches = get_batches(test_X, test_y, batch_size)\n",
    "    \n",
    "        #训练的迭代，保存训练损失\n",
    "        for batch_i in range(len(train_X) // batch_size):\n",
    "            x, y = next(train_batches)\n",
    "\n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: dropout_keep, #dropout_keep\n",
    "                lr: learning_rate}\n",
    "\n",
    "            step, train_loss, summaries, _ = sess.run([global_step, loss, train_summary_op, train_op], feed)  #cost\n",
    "            losses['train'].append(train_loss)\n",
    "            train_summary_writer.add_summary(summaries, step)  #\n",
    "            \n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * (len(train_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(train_X) // batch_size),\n",
    "                    train_loss))\n",
    "                \n",
    "        #使用测试数据的迭代\n",
    "        for batch_i  in range(len(test_X) // batch_size):\n",
    "            x, y = next(test_batches)\n",
    "            \n",
    "            categories = np.zeros([batch_size, 18])\n",
    "            for i in range(batch_size):\n",
    "                categories[i] = x.take(6,1)[i]\n",
    "\n",
    "            titles = np.zeros([batch_size, sentences_size])\n",
    "            for i in range(batch_size):\n",
    "                titles[i] = x.take(5,1)[i]\n",
    "\n",
    "            feed = {\n",
    "                uid: np.reshape(x.take(0,1), [batch_size, 1]),\n",
    "                user_gender: np.reshape(x.take(2,1), [batch_size, 1]),\n",
    "                user_age: np.reshape(x.take(3,1), [batch_size, 1]),\n",
    "                user_job: np.reshape(x.take(4,1), [batch_size, 1]),\n",
    "                movie_id: np.reshape(x.take(1,1), [batch_size, 1]),\n",
    "                movie_categories: categories,  #x.take(6,1)\n",
    "                movie_titles: titles,  #x.take(5,1)\n",
    "                targets: np.reshape(y, [batch_size, 1]),\n",
    "                dropout_keep_prob: 1,\n",
    "                lr: learning_rate}\n",
    "            \n",
    "            step, test_loss, summaries = sess.run([global_step, loss, inference_summary_op], feed)  #cost\n",
    "\n",
    "            #保存测试损失\n",
    "            losses['test'].append(test_loss)\n",
    "            inference_summary_writer.add_summary(summaries, step)  #\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if (epoch_i * (len(test_X) // batch_size) + batch_i) % show_every_n_batches == 0:\n",
    "                print('{}: Epoch {:>3} Batch {:>4}/{}   test_loss = {:.3f}'.format(\n",
    "                    time_str,\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    (len(test_X) // batch_size),\n",
    "                    test_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver.save(sess, save_dir)  #, global_step=epoch_i\n",
    "    print('Model Trained and Saved')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在 TensorBoard 中查看可视化结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensorboard --logdir /PATH_TO_CODE/runs/1513402825/summaries/\n",
    "\n",
    "<img src=\"assets/loss.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存参数\n",
    "保存`save_dir` 在生成预测时使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_params((save_dir))\n",
    "\n",
    "load_dir = load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显示训练Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 显示测试Loss\n",
    "迭代次数再增加一些，下降的趋势会明显一些"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses['test'], label='Test loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取 Tensors\n",
    "使用函数 [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name)从 `loaded_graph` 中获取tensors，后面的推荐功能要用到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    uid = loaded_graph.get_tensor_by_name(\"uid:0\")\n",
    "    user_gender = loaded_graph.get_tensor_by_name(\"user_gender:0\")\n",
    "    user_age = loaded_graph.get_tensor_by_name(\"user_age:0\")\n",
    "    user_job = loaded_graph.get_tensor_by_name(\"user_job:0\")\n",
    "    movie_id = loaded_graph.get_tensor_by_name(\"movie_id:0\")\n",
    "    movie_categories = loaded_graph.get_tensor_by_name(\"movie_categories:0\")\n",
    "    movie_titles = loaded_graph.get_tensor_by_name(\"movie_titles:0\")\n",
    "    targets = loaded_graph.get_tensor_by_name(\"targets:0\")\n",
    "    dropout_keep_prob = loaded_graph.get_tensor_by_name(\"dropout_keep_prob:0\")\n",
    "    lr = loaded_graph.get_tensor_by_name(\"LearningRate:0\")\n",
    "    #两种不同计算预测评分的方案使用不同的name获取tensor inference\n",
    "#     inference = loaded_graph.get_tensor_by_name(\"inference/inference/BiasAdd:0\")\n",
    "    inference = loaded_graph.get_tensor_by_name(\"inference/ExpandDims:0\") # 之前是MatMul:0 因为inference代码修改了 这里也要修改 感谢网友 @清歌 指出问题\n",
    "    movie_combine_layer_flat = loaded_graph.get_tensor_by_name(\"movie_fc/Reshape:0\")\n",
    "    user_combine_layer_flat = loaded_graph.get_tensor_by_name(\"user_fc/Reshape:0\")\n",
    "    return uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference, movie_combine_layer_flat, user_combine_layer_flat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指定用户和电影进行评分\n",
    "这部分就是对网络做正向传播，计算得到预测的评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rating_movie(user_id_val, movie_id_val):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "    \n",
    "        # Get Tensors from loaded model\n",
    "        uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, inference,_, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "    \n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = movies.values[movieid2idx[movie_id_val]][2]\n",
    "    \n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = movies.values[movieid2idx[movie_id_val]][1]\n",
    "    \n",
    "        feed = {\n",
    "              uid: np.reshape(users.values[user_id_val-1][0], [1, 1]),\n",
    "              user_gender: np.reshape(users.values[user_id_val-1][1], [1, 1]),\n",
    "              user_age: np.reshape(users.values[user_id_val-1][2], [1, 1]),\n",
    "              user_job: np.reshape(users.values[user_id_val-1][3], [1, 1]),\n",
    "              movie_id: np.reshape(movies.values[movieid2idx[movie_id_val]][0], [1, 1]),\n",
    "              movie_categories: categories,  #x.take(6,1)\n",
    "              movie_titles: titles,  #x.take(5,1)\n",
    "              dropout_keep_prob: 1}\n",
    "    \n",
    "        # Get Prediction\n",
    "        inference_val = sess.run([inference], feed)  \n",
    "    \n",
    "        return (inference_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_movie(234, 1401)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成Movie特征矩阵\n",
    "将训练好的电影特征组合成电影特征矩阵并保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "movie_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, movie_combine_layer_flat, __ = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in movies.values:\n",
    "        categories = np.zeros([1, 18])\n",
    "        categories[0] = item.take(2)\n",
    "\n",
    "        titles = np.zeros([1, sentences_size])\n",
    "        titles[0] = item.take(1)\n",
    "\n",
    "        feed = {\n",
    "            movie_id: np.reshape(item.take(0), [1, 1]),\n",
    "            movie_categories: categories,  #x.take(6,1)\n",
    "            movie_titles: titles,  #x.take(5,1)\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        movie_combine_layer_flat_val = sess.run([movie_combine_layer_flat], feed)  \n",
    "        movie_matrics.append(movie_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(movie_matrics).reshape(-1, 200)), open('movie_matrics.p', 'wb'))\n",
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_matrics = pickle.load(open('movie_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成User特征矩阵\n",
    "将训练好的用户特征组合成用户特征矩阵并保存到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graph = tf.Graph()  #\n",
    "users_matrics = []\n",
    "with tf.Session(graph=loaded_graph) as sess:  #\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    uid, user_gender, user_age, user_job, movie_id, movie_categories, movie_titles, targets, lr, dropout_keep_prob, _, __,user_combine_layer_flat = get_tensors(loaded_graph)  #loaded_graph\n",
    "\n",
    "    for item in users.values:\n",
    "\n",
    "        feed = {\n",
    "            uid: np.reshape(item.take(0), [1, 1]),\n",
    "            user_gender: np.reshape(item.take(1), [1, 1]),\n",
    "            user_age: np.reshape(item.take(2), [1, 1]),\n",
    "            user_job: np.reshape(item.take(3), [1, 1]),\n",
    "            dropout_keep_prob: 1}\n",
    "\n",
    "        user_combine_layer_flat_val = sess.run([user_combine_layer_flat], feed)  \n",
    "        users_matrics.append(user_combine_layer_flat_val)\n",
    "\n",
    "pickle.dump((np.array(users_matrics).reshape(-1, 200)), open('users_matrics.p', 'wb'))\n",
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_matrics = pickle.load(open('users_matrics.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始推荐电影\n",
    "使用生产的用户特征矩阵和电影特征矩阵做电影推荐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推荐同类型的电影\n",
    "思路是计算当前看的电影特征向量与整个电影特征矩阵的余弦相似度，取相似度最大的top_k个，这里加了些随机选择在里面，保证每次的推荐稍稍有些不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_same_type_movie(movie_id_val, top_k = 20):\n",
    "    \n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "        \n",
    "        norm_movie_matrics = tf.sqrt(tf.reduce_sum(tf.square(movie_matrics), 1, keep_dims=True))\n",
    "        normalized_movie_matrics = movie_matrics / norm_movie_matrics\n",
    "\n",
    "        #推荐同类型的电影\n",
    "        probs_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(normalized_movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_same_type_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推荐您喜欢的电影\n",
    "思路是使用用户特征向量与电影特征矩阵计算所有电影的评分，取评分最高的top_k个，同样加了些随机选择部分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_your_favorite_movie(user_id_val, top_k = 10):\n",
    "\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        #推荐您喜欢的电影\n",
    "        probs_embeddings = (users_matrics[user_id_val-1]).reshape([1, 200])\n",
    "\n",
    "        probs_similarity = tf.matmul(probs_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     print(sim.shape)\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "        \n",
    "    #     sim_norm = probs_norm_similarity.eval()\n",
    "    #     print((-sim_norm[0]).argsort()[0:top_k])\n",
    "    \n",
    "        print(\"以下是给您的推荐：\")\n",
    "        p = np.squeeze(sim)\n",
    "        p[np.argsort(p)[:-top_k]] = 0\n",
    "        p = p / np.sum(p)\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = np.random.choice(3883, 1, p=p)[0]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_your_favorite_movie(234, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 看过这个电影的人还看了（喜欢）哪些电影\n",
    "- 首先选出喜欢某个电影的top_k个人，得到这几个人的用户特征向量。\n",
    "- 然后计算这几个人对所有电影的评分\n",
    "- 选择每个人评分最高的电影作为推荐\n",
    "- 同样加入了随机选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def recommend_other_favorite_movie(movie_id_val, top_k = 20):\n",
    "    loaded_graph = tf.Graph()  #\n",
    "    with tf.Session(graph=loaded_graph) as sess:  #\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "        loader.restore(sess, load_dir)\n",
    "\n",
    "        probs_movie_embeddings = (movie_matrics[movieid2idx[movie_id_val]]).reshape([1, 200])\n",
    "        probs_user_favorite_similarity = tf.matmul(probs_movie_embeddings, tf.transpose(users_matrics))\n",
    "        favorite_user_id = np.argsort(probs_user_favorite_similarity.eval())[0][-top_k:]\n",
    "    #     print(normalized_users_matrics.eval().shape)\n",
    "    #     print(probs_user_favorite_similarity.eval()[0][favorite_user_id])\n",
    "    #     print(favorite_user_id.shape)\n",
    "    \n",
    "        print(\"您看的电影是：{}\".format(movies_orig[movieid2idx[movie_id_val]]))\n",
    "        \n",
    "        print(\"喜欢看这个电影的人是：{}\".format(users_orig[favorite_user_id-1]))\n",
    "        probs_users_embeddings = (users_matrics[favorite_user_id-1]).reshape([-1, 200])\n",
    "        probs_similarity = tf.matmul(probs_users_embeddings, tf.transpose(movie_matrics))\n",
    "        sim = (probs_similarity.eval())\n",
    "    #     results = (-sim[0]).argsort()[0:top_k]\n",
    "    #     print(results)\n",
    "    \n",
    "    #     print(sim.shape)\n",
    "    #     print(np.argmax(sim, 1))\n",
    "        p = np.argmax(sim, 1)\n",
    "        print(\"喜欢看这个电影的人还喜欢看：\")\n",
    "\n",
    "        results = set()\n",
    "        while len(results) != 5:\n",
    "            c = p[random.randrange(top_k)]\n",
    "            results.add(c)\n",
    "        for val in (results):\n",
    "            print(val)\n",
    "            print(movies_orig[val])\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_other_favorite_movie(1401, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论\n",
    "\n",
    "以上就是实现的常用的推荐功能，将网络模型作为回归问题进行训练，得到训练好的用户特征矩阵和电影特征矩阵进行推荐。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 扩展阅读\n",
    "如果你对个性化推荐感兴趣，以下资料建议你看看："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`Understanding Convolutional Neural Networks for NLP`](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)\n",
    "\n",
    "- [`Convolutional Neural Networks for Sentence Classification`](https://github.com/yoonkim/CNN_sentence)\n",
    "\n",
    "- [`利用TensorFlow实现卷积神经网络做文本分类`](http://www.jianshu.com/p/ed3eac3dcb39?from=singlemessage)\n",
    "\n",
    "- [`Convolutional Neural Network for Text Classification in Tensorflow`](https://github.com/dennybritz/cnn-text-classification-tf)\n",
    "\n",
    "- [`SVD Implement Recommendation systems`](https://github.com/songgc/TF-recomm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今天的分享就到这里，请多指教！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
